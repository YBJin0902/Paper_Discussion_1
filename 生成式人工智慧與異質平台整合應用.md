# 生成式人工智慧與異質平台整合應用


## 人工智慧的發展歷程

早期的電腦系統主要依賴**程式語言與規則邏輯**進行推理，在特定情境下依照預先設定的規則做出反應。  
到了 **1980 年代之後**，隨著**大數據與統計學習方法**的興起，電腦開始能從資料中發現規律與趨勢，正式進入**機器學習時代**。

近年來，生成式人工智慧的出現，開啟了**第三階段的 AI 革命**——  
AI 不僅能理解資料，還能**主動生成新的內容**，包括文字、影像與程式碼。

---

## 生成式 AI 的核心模型

### 1. GAN（Generative Adversarial Network）

- 由**生成器（Generator）**與**判別器（Discriminator）**組成  
- 生成器負責產生擬真的假資料  
- 判別器負責判斷資料真偽  
- 透過雙方不斷競爭與學習，使生成結果愈來愈逼真  

---

### 2. Transformer 架構

- 為目前**大型語言模型（LLM）**（如 GPT 系列）的基礎  
- 核心為 **自注意力機制（Self-Attention）**
- 能有效理解長距離語意關係
- 使模型能產生具邏輯性、連貫性的長篇文字

---

## GPT 系列模型的演進

- **GPT-1（2018）**  
  - 使用約 7000 本書籍作為訓練資料  

- **GPT-2**  
  - 擴大資料集，展現初步語意理解能力  

- **GPT-3**  
  - 具備成熟的文字生成能力  
  - 成為 ChatGPT 誕生的重要基礎  

- **GPT-4**  
  - 支援多模態學習（文字＋影像）  

- **GPT-5（現階段）**  
  - 被定位為更聰明、快速且實用的模型  
  - 可處理程式設計、數學、寫作、健康知識等多領域任務  
  - 錯誤率進一步降低  

---

## ChatGPT 能與人自然對話的關鍵

楊振坤教授說明，ChatGPT 背後的核心技術為：

### 強化學習與人類回饋（RLHF）

- 模型先從大量範例中學習人類期望的回答方式  
- 再由人類標註者對模型輸出進行排序  
- 建立獎勵模型後，透過強化學習不斷優化  
- 使模型逐漸學會人類的語言邏輯與溝通習慣  
- 最終能像人類一樣自然地回應問題  

---

## 生成式 AI 面臨的挑戰

### 1. 高昂的訓練成本
- GPT-3 約使用 **1000 張 A100 GPU** 訓練超過兩個月  
- GPT-4 動用超過 **8000 張 H100 GPU**，訓練成本極高  

### 2. 資源集中化問題
- 僅大型企業或雲端平台（如 Azure、AWS、Google Colab）  
  才有能力支撐如此龐大的運算環境  

### 3. 能源消耗與環境影響
- 訓練過程消耗大量電力  
- 帶來碳排放與永續發展問題  
- 如何在效能與環境影響間取得平衡，是未來的重要課題  

---